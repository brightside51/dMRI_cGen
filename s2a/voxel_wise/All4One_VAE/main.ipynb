{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# *Initial* **Setup**\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Library** *Settings*\n",
    "\n",
    "The Real Package Name must be found in https://pypi.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pfernan2\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pl_bolts\\callbacks\\data_monitor.py:20: UnderReviewWarning: The feature warn_missing_pkg is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  warn_missing_pkg(\"wandb\")\n",
      "C:\\Users\\pfernan2\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pl_bolts\\models\\self_supervised\\amdim\\amdim_module.py:35: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
      "C:\\Users\\pfernan2\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pl_bolts\\models\\self_supervised\\amdim\\amdim_module.py:93: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
      "C:\\Users\\pfernan2\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pl_bolts\\losses\\self_supervised_learning.py:234: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.nce_loss = AmdimNCELoss(tclip)\n",
      "C:\\Users\\pfernan2\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pl_bolts\\datamodules\\experience_source.py:18: UnderReviewWarning: The feature warn_missing_pkg is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  warn_missing_pkg(\"gym\")\n"
     ]
    }
   ],
   "source": [
    "# Library Import\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "import pickle\n",
    "import psutil\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchsummary\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "import pl_bolts                     # lightning_bolts\n",
    "import tensorboard\n",
    "import tensorflow as tf\n",
    "import fvcore\n",
    "import matplotlib.pyplot as plt\n",
    "import itk\n",
    "import itkwidgets\n",
    "import time\n",
    "import timeit\n",
    "import warnings\n",
    "import alive_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functionality Import\n",
    "from pathlib import Path\n",
    "from typing import List, Literal, Optional, Callable, Dict, Literal, Optional, Union, Tuple\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torchsummary import summary\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pl_bolts.models.autoencoders.components import resnet18_encoder, resnet18_decoder\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from PIL import Image\n",
    "from ipywidgets import interactive, IntSlider\n",
    "from tabulate import tabulate\n",
    "from alive_progress import alive_bar\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Control** *Station*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Parametrizations Parser Initialization\n",
    "data_parser = argparse.ArgumentParser(\n",
    "        description = \"2/3D MUDI Dataset Settings\")\n",
    "data_parser.add_argument(                               # Dataset Version Variable\n",
    "        '--version', type = int,                        # Default: 0\n",
    "        default = 3,\n",
    "        help = \"Dataset Save Version\")\n",
    "data_parser.add_argument(                               # Dataset Dimensionality\n",
    "        '--dim', type = int,                            # Default: 3\n",
    "        default = 2,\n",
    "        help = \"Dataset Dimensionality\")\n",
    "data_parser.add_argument(                               # Dataset Batch Size Value\n",
    "        '--batch_size', type = int,                     # Default: 500\n",
    "        default = 500,\n",
    "        help = \"Dataset Batch Size Value\")\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "# Dataset Label Parametrization Arguments\n",
    "data_parser.add_argument(                       # Control Variable for the Inclusion of Patient ID in Labels\n",
    "        '--patient_id', type = bool,            # Default: True\n",
    "        default = False,\n",
    "        help = \"Control Variable for the Inclusion of Patient ID in Labels\")\n",
    "data_parser.add_argument(                       # Control Variable for the Conversion of 3 Gradient Directions\n",
    "        '--gradient_coord', type = bool,        # Coordinates into 2 Gradient Direction Angles (suggested by prof. Chantal)\n",
    "        default = False,                        # Default: True (3 Coordinate Gradient Values)\n",
    "        help = \"Control Variable for the Conversion of Gradient Direction Mode\")\n",
    "data_parser.add_argument(                       # Control Variable for the Rescaling & Normalization of Labels\n",
    "        '--label_norm', type = bool,            # Default: True\n",
    "        default = True,\n",
    "        help = \"Control Variable for the Rescaling & Normalization of Labels\")\n",
    "data_settings = data_parser.parse_args(\"\")\n",
    "num_labels = 7\n",
    "if not(data_settings.patient_id): num_labels -= 1           # Exclusion of Patiend ID\n",
    "if not(data_settings.gradient_coord): num_labels -= 1       # Conversion of Gradient Coordinates to Angles\n",
    "data_parser.add_argument(                                   # Dataset Number of Labels\n",
    "        '--num_labels', type = int,                         # Default: 7\n",
    "        default = num_labels,\n",
    "        help = \"MUDI Dataset Number of Labels\")\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "# Addition of File & Folderpath Arguments\n",
    "data_parser.add_argument(                               # Path for Main Dataset Folder\n",
    "        '--main_folderpath', type = str,\n",
    "        default = '../../../Datasets/MUDI Dataset',\n",
    "        help = 'Main Folderpath for Root Dataset')\n",
    "data_settings = data_parser.parse_args(\"\")\n",
    "data_parser.add_argument(                               # Path for Folder Containing Patient Data Files\n",
    "        '--patient_folderpath', type = Path,\n",
    "        default = Path(f'{data_settings.main_folderpath}/Patient Data'),\n",
    "        help = 'Input Folderpath for Segregated Patient Data')\n",
    "data_parser.add_argument(                               # Path for Folder Containing Mask Data Files\n",
    "        '--mask_folderpath', type = Path,\n",
    "        default = Path(f'{data_settings.main_folderpath}/Patient Mask'),\n",
    "        help = 'Input Folderpath for Segregated Patient Mask Data')\n",
    "data_parser.add_argument(                               # Path for Parameter Value File\n",
    "        '--param_filepath', type = Path,\n",
    "        default = Path(f'{data_settings.main_folderpath}/Raw Data/parameters_new.xlsx'),\n",
    "        help = 'Input Filepath for Parameter Value Table')\n",
    "data_parser.add_argument(                               # Path for Patient Information File\n",
    "        '--info_filepath', type = Path,\n",
    "        default = Path(f'{data_settings.main_folderpath}/Raw Data/header1_.csv'),\n",
    "        help = 'Input Filepath for Patient Information Table')\n",
    "data_parser.add_argument(                               # Path for Dataset Saved Files\n",
    "        '--save_folderpath', type = Path,\n",
    "        default = Path(f'{data_settings.main_folderpath}/Saved Data/V{data_settings.version}'),\n",
    "        help = 'Output Folderpath for MUDI Dataset Saved Versions')\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "# Dataset Splitting Arguments\n",
    "data_parser.add_argument(                       # Number of Patients to be used in the Test Set\n",
    "        '--test_patients', type = int,          # Default: 1\n",
    "        default = 1,\n",
    "        help = \"Number of Patients in Test Set\")\n",
    "data_parser.add_argument(                       # Number / Percentage of Parameters for Training Set's Training\n",
    "        '--train_params', type = int,           # Default: 500\n",
    "        default = 500,\n",
    "        help = \"Number / Percentage of Patients in the Training of the Training Set\")\n",
    "data_parser.add_argument(                       # Number / Percentage of Parameters for Training Set's Training\n",
    "        '--test_params', type = int,            # Default: 20\n",
    "        default = 20,\n",
    "        help = \"Number / Percentage of Patients in the Training of the Test Set\")\n",
    "\n",
    "# Boolean Control Input & Shuffling Arguments\n",
    "data_parser.add_argument(                       # Control Variable for the Usage of Percentage Values in Parameters\n",
    "        '--percentage', type = bool,            # Default: False\n",
    "        default = False,\n",
    "        help = \"Control Variable for the Usage of Percentage Values in Parameters\")\n",
    "data_parser.add_argument(                       # Ability to Shuffle the Patients that compose both Training and Test Sets\n",
    "        '--patient_shuffle', type = bool,       # Default: False\n",
    "        default = False,\n",
    "        help = \"Ability to Shuffle the Patients that compose both Training and Test Sets\")\n",
    "data_parser.add_argument(                       # Ability to Shuffle the Samples inside both Training and Validation Sets\n",
    "        '--sample_shuffle', type = bool,        # Default: False\n",
    "        default = False,\n",
    "        help = \"Ability to Shuffle the Samples inside both Training and Validation Sets\")\n",
    "data_parser.add_argument(                       # Number of Workers for DataLoader Usage\n",
    "        '--num_workers', type = int,                # Default: 1\n",
    "        default = 20,\n",
    "        help = \"Number of Workers for DataLoader Usage\")\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "# Dataset Pre-Processing Arguments\n",
    "data_parser.add_argument(                       # Data Pre-Processing Method\n",
    "        '--pre_processing', type = str,         # Default: Zero Padding\n",
    "        default = 'Zero Padding',\n",
    "        choices = ['Interpolation', 'Zero Padding', 'CNN'],\n",
    "        help = \"Data Pre-Processing Method\")\n",
    "data_parser.add_argument(                       # Final 3D Image Shape for Pre-Processing Output\n",
    "        '--img_shape', type = np.array,         # Default: [85, 128, 128]\n",
    "        default = np.array((60, 96, 96)),\n",
    "        help = \"Final 3D Image Shape for Pre-Processing Output\")\n",
    "data_parser.add_argument(                       # Number of Selected Slices in 2D Conversion\n",
    "        '--num_slices', type = int,             # Default: 35\n",
    "        default = 35,\n",
    "        help = \"Number of Selected Slices in 2D Conversion\")\n",
    "data_settings = data_parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All4One 2D VAE Model Parametrizations Parser Initialization\n",
    "model_parser = argparse.ArgumentParser(\n",
    "        description = \"All4One 2D VAE Settings\")\n",
    "model_parser.add_argument(              # Model Version Variable\n",
    "        '--model_version', type = int,  # Default: 0\n",
    "        default = 2,\n",
    "        help = \"Experiment Version\")\n",
    "model_parser.add_argument(              # Dataset Version Variable\n",
    "        '--data_version', type = int,   # Default: 0\n",
    "        default = 3,\n",
    "        help = \"MUDI Dataset Version\")\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "# Addition of Filepath Arguments\n",
    "model_parser.add_argument(\n",
    "        '--reader_folderpath', type = Path,\n",
    "        default = Path(f'{data_settings.main_folderpath}/Dataset Reader'),\n",
    "        help = 'Input Folderpath for MUDI Dataset Reader')\n",
    "model_parser.add_argument(\n",
    "        '--data_folderpath', type = Path,\n",
    "        default = Path(f'{data_settings.main_folderpath}/Saved Data/V{data_settings.version}'),\n",
    "        help = 'Input Folderpath for MUDI Dataset Saved Versions')\n",
    "model_parser.add_argument(\n",
    "        '--model_folderpath', type = str,\n",
    "        default = 'Model Builds',\n",
    "        help = 'Input Folderpath for Model Build & Architecture')\n",
    "model_parser.add_argument(\n",
    "        '--script_folderpath', type = str,\n",
    "        default = 'Training Scripts',\n",
    "        help = 'Input Folderpath for Training & Testing Script Functions')\n",
    "model_parser.add_argument(\n",
    "        '--save_folderpath', type = str,\n",
    "        default = 'Saved Models',\n",
    "        help = 'Output Folderpath for Saved & Saving Models')\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "# Addition of Training Requirement Arguments\n",
    "model_parser.add_argument(              # Number of Epochs\n",
    "        '--num_epochs', type = int,     # Default: 1\n",
    "        default = 50,\n",
    "        help = \"Number of Epochs in Training Mode\")\n",
    "model_parser.add_argument(              # Base Learning Rate\n",
    "        '--base_lr', type = float,      # Default: 1e-4\n",
    "        default = 1e-4,\n",
    "        help = \"Base Learning Rate Value in Training Mode\")\n",
    "model_parser.add_argument(              # Weight Decay Value\n",
    "        '--weight_decay', type = float, # Default: 0.0001\n",
    "        default = 1e-4,\n",
    "        help = \"Weight Decay Value in Training Mode\")\n",
    "model_parser.add_argument(              # Learning Rate Decay Ratio\n",
    "        '--lr_decay', type = float,     # Default: 0.9\n",
    "        default = 0.9,\n",
    "        help = \"Learning Rate Decay Value in Training Mode\")\n",
    "        \n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "# Addition of Model Architecture Arguments\n",
    "model_parser.add_argument(              # Dataset Number of Labels\n",
    "        '--num_labels', type = int,     # Default: 7\n",
    "        default = data_settings.num_labels,\n",
    "        help = \"MUDI Dataset Number of Labels\")\n",
    "model_parser.add_argument(              # Latent Space Dimensionality\n",
    "        '--latent_dim', type = int,     # Default: 64\n",
    "        default = 128,\n",
    "        help = \"Latent Space Dimensionality Value\")\n",
    "model_parser.add_argument(              # Convolutional Layer Expansion Value\n",
    "        '--expansion', type = int,      # Default: 1\n",
    "        default = 1,\n",
    "        help = \"Convolutional Layer Expansion Value\")\n",
    "model_parser.add_argument(              # Kullback-Leibler Loss Weight\n",
    "        '--kl_alpha', type = int,       # Default: 1\n",
    "        default = 1,\n",
    "        help = \"Kullback-Leibler Loss Weight\")\n",
    "if data_settings.dim == 2: num_channel = 1\n",
    "else: num_channel = data_settings.num_slices\n",
    "model_parser.add_argument(              # Number of Channels in given 3D Image Batch\n",
    "        '--num_channel', type = int,    # Default: 1 (for 2D MUDI Dataset)\n",
    "        default = num_channel,\n",
    "        help = \"Number of Channels in given 3D Image Batch\")\n",
    "model_parser.add_argument(              # Image Side Length (No Support for non-Square Images)\n",
    "        '--img_shape', type = int,      # Default: 128 \n",
    "        default = data_settings.img_shape[-1],\n",
    "        help = \"Image Side Length\")\n",
    "\n",
    "model_settings = model_parser.parse_args(\"\")\n",
    "model_settings.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# *Model* **Building**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "# --------------------------------------- Encoder Build --------------------------------------\n",
    "##############################################################################################\n",
    "\n",
    "# Main Encoder Block Construction Class\n",
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    # Constructor / Initialization Function\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channel: int,        # Number of Encoder Block's Convolutional Input Channels\n",
    "        stride: int = 1,\n",
    "        padding: int = 1\n",
    "    ):\n",
    "\n",
    "        # Main Block's Downsampling Architecture Definition\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        out_channel = in_channel * stride\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(      in_channel, out_channel, kernel_size = 3,\n",
    "                            stride = stride, padding = padding, bias = False),\n",
    "            nn.BatchNorm2d( out_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(      out_channel, out_channel, kernel_size = 3,\n",
    "                            stride = 1, padding = padding, bias = False),\n",
    "            nn.BatchNorm2d( out_channel))\n",
    "\n",
    "        # Main Block's Shorcut Architecture Definition\n",
    "        if stride == 1: self.shortcut = nn.Sequential()\n",
    "        else:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(      in_channel, out_channel, kernel_size = 1,\n",
    "                                stride = stride, bias = False),\n",
    "                nn.BatchNorm2d( out_channel))\n",
    "\n",
    "    # Main Block Application Function\n",
    "    def forward(self, X):\n",
    "\n",
    "        # Main Block Architecture Walkthrough\n",
    "        out = self.block(X)\n",
    "        out = out + self.shortcut(X)\n",
    "        return F.relu(out)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "# Encoder Model Class\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    # Constructor / Initialization Function\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channel: int = 64,               # Number of Input Channels in ResNet Main Block Intermediate Layers' Blocks\n",
    "        num_channel: int = 1,               # Number of Channels in each Image (Default: 1 for 2D Dataset)\n",
    "        latent_dim: int = 64,               # Latent Space Dimensionality\n",
    "        expansion: int = 1,                 # Expansion Factor for Stride Value in ResNet Main Block Intermediate Layers\n",
    "        num_blocks: list = [2, 2, 2, 2]     # Number of Blocks in ResNet Main Block Intermediate Layers\n",
    "    ):\n",
    "\n",
    "        # Class Variable Logging\n",
    "        super(Encoder, self).__init__()\n",
    "        assert(len(num_blocks) == 4), \"Number of Blocks provided Not Supported!\"\n",
    "        self.in_channel = in_channel; self.num_channel = num_channel\n",
    "        self.latent_dim = latent_dim; self.expansion = expansion\n",
    "\n",
    "        # Encoder Downsampling Architecture Definition\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(      self.num_channel, 64, kernel_size = 3,\n",
    "                            stride = 2, padding = 1, bias = False),\n",
    "            nn.BatchNorm2d( 64),\n",
    "            nn.ReLU(),\n",
    "            self.main_layer(out_channel = 64, num_blocks = num_blocks[0], stride = 1),\n",
    "            self.main_layer(out_channel = 128, num_blocks = num_blocks[1], stride = 2),\n",
    "            self.main_layer(out_channel = 256, num_blocks = num_blocks[2], stride = 2),\n",
    "            self.main_layer(out_channel = 512, num_blocks = num_blocks[3], stride = 2))\n",
    "        self.linear = nn.Linear(512, 2 * self.latent_dim)\n",
    "\n",
    "    # Encoder Repeatable Layer Definition Function\n",
    "    def main_layer(\n",
    "        self,\n",
    "        out_channel: int,\n",
    "        num_blocks: int,\n",
    "        stride: int = 2\n",
    "    ):\n",
    "\n",
    "        # Layer Architecture Creation\n",
    "        stride = [stride] + [1] * (num_blocks - 1); layer = []\n",
    "        for s in stride:\n",
    "            layer.append(EncoderBlock(self.in_channel, stride = s))\n",
    "            self.in_channel = out_channel\n",
    "        return nn.Sequential(*layer)\n",
    "    \n",
    "    # Encoder Application Function\n",
    "    def forward(\n",
    "        self,\n",
    "        X: np.ndarray or torch.Tensor       # 3D Image Input\n",
    "    ):\n",
    "\n",
    "        # Forwad Propagation in Encoder Architecture\n",
    "        X = torch.Tensor(X)\n",
    "        out = self.net(X)\n",
    "        out = F.adaptive_avg_pool2d(out, 1)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        mu = out[:, :self.latent_dim]\n",
    "        var = out[:, self.latent_dim:]\n",
    "        return mu, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "# --------------------------------------- Decoder Build --------------------------------------\n",
    "##############################################################################################\n",
    "\n",
    "# 2D Resizing Convolution\n",
    "class ResizeConv2d(nn.Module):\n",
    "\n",
    "    # Constructor / Initialization Function\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channel: int,\n",
    "        out_channel: int,\n",
    "        kernel_size: int,\n",
    "        scale_factor: int,\n",
    "        mode: str ='nearest'\n",
    "    ):\n",
    "\n",
    "        # 2D Resizing Convolution \n",
    "        super(ResizeConv2d, self).__init__()\n",
    "        self.scale_factor = scale_factor; self.mode = mode\n",
    "        self.conv = nn.Conv2d(  in_channel, out_channel,\n",
    "                                kernel_size, stride = 1, padding = 1)\n",
    "\n",
    "    # Resizing Convolutional Block Application Function\n",
    "    def forward(\n",
    "        self,\n",
    "        X: np.ndarray or torch.Tensor       # 3D Image Input\n",
    "    ):\n",
    "        \n",
    "        # Resizing Convolutional Block Architecture Walkthrough\n",
    "        X = torch.Tensor(X)\n",
    "        out = F.interpolate(X, scale_factor = self.scale_factor,\n",
    "                            mode = self.mode)\n",
    "        return self.conv(out)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "# Main Decoder Block Construction Class\n",
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    # Constructor / Initialization Function\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channel: int,        # Number of Decoder Block's Convolutional Input Channels\n",
    "        stride: int = 1,\n",
    "        padding: int = 1\n",
    "    ):\n",
    "\n",
    "        # Main Block's Upsampling Architecture Definition\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        out_channel = int(in_channel / stride)\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(      in_channel, in_channel, kernel_size = 3,\n",
    "                            stride = 1, padding = padding, bias = False),\n",
    "            nn.BatchNorm2d( in_channel),\n",
    "            nn.ReLU())\n",
    "\n",
    "        # Main Block's Shorcut Architecture Definition\n",
    "        if stride == 1:\n",
    "            self.block2 = nn.Sequential(\n",
    "                nn.Conv2d(      in_channel, out_channel, kernel_size = 3,\n",
    "                                stride = 1, padding = padding, bias = False),\n",
    "                nn.BatchNorm2d( out_channel))\n",
    "            self.shortcut = nn.Sequential()\n",
    "        else:\n",
    "            self.block2 = nn.Sequential(\n",
    "                ResizeConv2d(   in_channel, out_channel,\n",
    "                                kernel_size = 3, scale_factor = stride),\n",
    "                nn.BatchNorm2d( out_channel))\n",
    "            self.shortcut = self.block2\n",
    "\n",
    "    # Main Block Application Function\n",
    "    def forward(\n",
    "        self,\n",
    "        X: np.ndarray or torch.Tensor       # 3D Image Input\n",
    "    ):\n",
    "\n",
    "        # Main Block Architecture Walkthrough\n",
    "        out = self.block1(X)\n",
    "        out = self.block2(out)\n",
    "        out = out + self.shortcut(X)\n",
    "        return F.relu(out)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "# Decoder Model Class\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    # Constructor / Initialization Function\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_channel: int = 1,               # Number of Channels in each Image (Default: 1 for 2D Dataset)\n",
    "        img_shape: int = 96,                # Square Image Side Length (1/4th of pre-Convolution No. Channels)\n",
    "        latent_dim: int = 64,               # Latent Space Dimensionality\n",
    "        expansion: int = 1,                 # Expansion Factor for Stride Value in ResNet Main Block Intermediate Layers\n",
    "        num_blocks: list = [2, 2, 2, 2]     # Number of Blocks in ResNet Main Block Intermediate Layers\n",
    "    ):\n",
    "\n",
    "        # Class Variable Logging\n",
    "        super(Decoder, self).__init__()\n",
    "        assert(len(num_blocks) == 4), \"Number of Blocks provided Not Supported!\"\n",
    "        self.num_blocks = num_blocks\n",
    "        self.in_channel = img_shape * (2 ** (len(self.num_blocks)))\n",
    "        self.channel = self.in_channel; self.num_channel = num_channel\n",
    "        self.img_shape = img_shape; self.latent_dim = latent_dim\n",
    "\n",
    "        # Decoder Upsampling Architecture Definition\n",
    "        self.linear = nn.Linear(self.latent_dim, self.in_channel)\n",
    "        self.net = nn.Sequential(\n",
    "            self.main_layer(out_channel = int(self.channel / 2), num_blocks = num_blocks[3], stride = 2),\n",
    "            self.main_layer(out_channel = int(self.channel / 2), num_blocks = num_blocks[2], stride = 2),\n",
    "            self.main_layer(out_channel = int(self.channel / 2), num_blocks = num_blocks[1], stride = 2),\n",
    "            self.main_layer(out_channel = int(self.channel), num_blocks = num_blocks[0], stride = 1),\n",
    "            nn.Sigmoid(),\n",
    "            ResizeConv2d(   self.img_shape * 2, self.num_channel,\n",
    "                            kernel_size = 3, scale_factor = img_shape / 64))\n",
    "\n",
    "    # Decoder Repeatable Layer Definition Function\n",
    "    def main_layer(\n",
    "        self,\n",
    "        out_channel: int,\n",
    "        num_blocks: int,\n",
    "        stride: int = 2\n",
    "    ):\n",
    "\n",
    "        # Layer Architecture Creation\n",
    "        stride = [stride] + [1] * (num_blocks - 1); layer = []\n",
    "        for s in reversed(stride):\n",
    "            layer.append(DecoderBlock(self.channel, stride = s))\n",
    "        self.channel = out_channel\n",
    "        return nn.Sequential(*layer)\n",
    "\n",
    "    # Decoder Application Function\n",
    "    def forward(\n",
    "        self,\n",
    "        z: np.ndarray or torch.Tensor       # 3D Latent Representation Input\n",
    "    ):\n",
    "\n",
    "        # Forward Propagation in Decoder Architecture\n",
    "        z = torch.Tensor(z)\n",
    "        out = self.linear(z)\n",
    "        out = out.view(z.size(0), self.in_channel, 1, 1)\n",
    "        out = F.interpolate(out, scale_factor = 2 ** (len(self.num_blocks) - 1))\n",
    "        out = self.net(out)\n",
    "        out = out.view( z.size(0), self.num_channel,\n",
    "                        self.img_shape, self.img_shape)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "# ------------------------------------- All4One VAE Build ------------------------------------\n",
    "##############################################################################################\n",
    "\n",
    "# VAE Model Class\n",
    "class All4One(nn.Module):\n",
    "\n",
    "    # Constructor / Initialization Function\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim: int = 64,               # Latent Space Dimensionality\n",
    "        num_channel: int = 1,               # Number of Channels in each Image (Default: 1 for 2D Dataset)\n",
    "        img_shape: int = 96,                # Square Image Side Length (1/4th of pre-Convolution No. Channels)\n",
    "        in_channel: int = 64,               # Number of Input Channels in ResNet Main Block Intermediate Layers' Blocks\n",
    "        expansion: int = 1,                 # Expansion Factor for Stride Value in ResNet Main Block Intermediate Layers\n",
    "        num_blocks: list = [2, 2, 2, 2]     # Number of Blocks in ResNet Main Block Intermediate Layers\n",
    "    ):\n",
    "\n",
    "        # Encoder & Decoder Construction\n",
    "        super(All4One, self).__init__()\n",
    "        self.encoder = Encoder(in_channel, num_channel, latent_dim, expansion, num_blocks)\n",
    "        self.decoder = Decoder(num_channel, img_shape, latent_dim, expansion, num_blocks)\n",
    "\n",
    "    # Latent Space Reparametrization\n",
    "    @staticmethod\n",
    "    def reparam(mean, logvar):\n",
    "        std = torch.exp(logvar / 2)\n",
    "        epsilon = torch.randn_like(std)\n",
    "        return epsilon * std + mean\n",
    "\n",
    "    # All4One VAE Application Function\n",
    "    def forward(\n",
    "        self,\n",
    "        X: np.ndarray or torch.Tensor       # 3D Latent Representation Input\n",
    "    ):\n",
    "\n",
    "        # Forward Propagation in VAE Architecture\n",
    "        mu, var = self.encoder(X)\n",
    "        z = self.reparam(mu, var)\n",
    "        X_fake = self.decoder(z)\n",
    "        return mu, var, z, X_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1536, 8, 8])\n",
      "torch.Size([5, 1, 96, 96])\n",
      "torch.Size([5, 1, 96, 96])\n"
     ]
    }
   ],
   "source": [
    "# Encoder Initialization Example\n",
    "enc = Encoder()\n",
    "#summary(enc, (1, 96, 96))\n",
    "X = torch.rand(5, 1, 96, 96)\n",
    "mu, var = enc(X)\n",
    "\n",
    "def reparameterize(mean, logvar):\n",
    "    std = torch.exp(logvar / 2) # in log-space, squareroot is divide by two\n",
    "    epsilon = torch.randn_like(std)\n",
    "    return epsilon * std + mean\n",
    "\n",
    "# Decoder Initialization Example\n",
    "z = reparameterize(mu, var)\n",
    "dec = Decoder()\n",
    "X_fake = dec(z)\n",
    "print(X_fake.shape)\n",
    "#summary(dec, (5, 128))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Running** *Scripts*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE Model Training, Validation & Testing Script Class\n",
    "class LitAll4One(pl.LightningModule):\n",
    "\n",
    "    ##############################################################################################\n",
    "    # ----------------------------------- Model & Dataset Setup ----------------------------------\n",
    "    ##############################################################################################\n",
    "\n",
    "    # Constructor / Initialization Function\n",
    "    def __init__(\n",
    "        self,\n",
    "        settings: argparse.ArgumentParser,              # Model Settings & Parametrizations\n",
    "    ):\n",
    "\n",
    "        # Class Variable Logging\n",
    "        super().__init__()\n",
    "        self.settings = settings\n",
    "        self.lr_decay_epochs = [80, 140]                # Epochs for Learning Rate Decay\n",
    "\n",
    "        # Model Initialization\n",
    "        self.model = All4One(               latent_dim = settings.latent_dim,\n",
    "                                            num_channel = settings.num_channel,\n",
    "                                            img_shape = settings.img_shape,\n",
    "                                            expansion = settings.expansion)\n",
    "        self.optimizer = torch.optim.Adam(  self.model.parameters(),\n",
    "                                            lr = self.settings.base_lr,\n",
    "                                            weight_decay = self.settings.weight_decay, )\n",
    "        self.recon_criterion = nn.MSELoss(); self.past_epochs = 0\n",
    "\n",
    "        # Existing Model Checkpoint Loading\n",
    "        self.model_filepath = Path(f\"{self.settings.save_folderpath}/V{self.settings.model_version}/All4One (V{self.settings.model_version}).pth\")\n",
    "        if self.settings.model_version != 0 and self.model_filepath.exists():\n",
    "\n",
    "            # Checkpoint Fixing (due to the use of nn.DataParallel)\n",
    "            print(f\"DOWNLOADING All4One 2D VAE (Version {self.settings.model_version})\")\n",
    "            checkpoint = torch.load(self.model_filepath); self.checkpoint_fix = dict()\n",
    "            for sd, sd_value in checkpoint.items():\n",
    "                if sd == 'ModelSD' or sd == 'OptimizerSD':\n",
    "                    self.checkpoint_fix[sd] = OrderedDict()\n",
    "                    for key, value in checkpoint[sd].items():\n",
    "                        if key[0:7] == 'module.':\n",
    "                            self.checkpoint_fix[sd][key[7:]] = value\n",
    "                        else: self.checkpoint_fix[sd][key] = value\n",
    "                else: self.checkpoint_fix[sd] = sd_value\n",
    "            \n",
    "            # Application of Checkpoint's State Dictionary\n",
    "            self.model.load_state_dict(self.checkpoint_fix['ModelSD'])\n",
    "            self.optimizer.load_state_dict(self.checkpoint_fix['OptimizerSD'])\n",
    "            self.past_epochs = self.checkpoint_fix['Training Epochs']\n",
    "            torch.set_rng_state(self.checkpoint_fix['RNG State'])\n",
    "            del checkpoint\n",
    "        self.lr_schedule = torch.optim.lr_scheduler.ExponentialLR(  self.optimizer,     # Learning Rate Decay\n",
    "                                                    gamma = self.settings.lr_decay)     # in Chosen Epochs\n",
    "        self.model = nn.DataParallel(self.model.to(self.settings.device))\n",
    "        \n",
    "    # Optimizer Initialization Function\n",
    "    def configure_optimizers(self): return super().configure_optimizers()\n",
    "\n",
    "    # Foward Functionality\n",
    "    def forward(self, X): return self.model(X)\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Train Set DataLoader Download\n",
    "    def train_dataloader(self):\n",
    "        TrainTrainLoader = v3DMUDI.loader(  Path(f\"{self.settings.data_folderpath}\"),\n",
    "                                            dim = 2, version = self.settings.data_version,\n",
    "                                            set_ = 'Test', mode_ = 'Train')\n",
    "                                            #set_ = 'Train', mode_ = 'Train')\n",
    "        self.train_batches = len(TrainTrainLoader)\n",
    "        return TrainTrainLoader\n",
    "    \n",
    "    # Validation Set DataLoader Download\n",
    "    def val_dataloader(self):\n",
    "        TrainValLoader = v3DMUDI.loader(Path(f\"{self.settings.data_folderpath}\"),\n",
    "                                        dim = 2, version = self.settings.data_version,\n",
    "                                        set_ = 'Test', mode_ = 'Train')\n",
    "                                        #set_ = 'Train', mode_ = 'Val')\n",
    "        self.val_batches = len(TrainValLoader)\n",
    "        return TrainValLoader\n",
    "\n",
    "    # Test Set DataLoader Download\n",
    "    def test_dataloader(self):\n",
    "        TestValLoader = v3DMUDI.loader( Path(f\"{self.settings.data_folderpath}\"),\n",
    "                                        dim = 2, version = self.settings.data_version,\n",
    "                                        set_ = 'Test', mode_ = 'Val')\n",
    "        self.test_batches = len(TestValLoader)\n",
    "        return TestValLoader\n",
    "\n",
    "    ##############################################################################################\n",
    "    # ------------------------------------- Training Script --------------------------------------\n",
    "    ##############################################################################################\n",
    "\n",
    "    # Functionality called upon the Start of Training\n",
    "    def on_train_start(self):\n",
    "        \n",
    "        # Model Training Mode Setup\n",
    "        self.model.train()\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        # TensorBoard Logger Initialization\n",
    "        self.train_logger = TensorBoardLogger(f'{self.settings.save_folderpath}/V{self.settings.model_version}', 'Training Performance')\n",
    "\n",
    "    # Functionality called upon the Start of Training Epoch\n",
    "    def on_train_epoch_start(self):\n",
    "        self.train_loss = 0\n",
    "        self.train_kl_loss = 0\n",
    "        self.train_recon_loss = 0\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Training Step / Batch Loop \n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        # Data Handling\n",
    "        X_batch, ygt_batch = batch\n",
    "        X_batch = X_batch.type(torch.float).to(self.settings.device)\n",
    "        #ygt_batch = ygt_batch.type(torch.float).to(self.settings.device)\n",
    "\n",
    "        # Forward Propagation & Loss Computation\n",
    "        mu_batch, var_batch, z_batch, X_fake_batch = self.model(X_batch)\n",
    "        kl_loss =  (-0.5 * (1 + var_batch - mu_batch ** 2 - torch.exp(var_batch)).sum(dim = 1)).mean(dim = 0)        \n",
    "        recon_loss = self.recon_criterion(X_fake_batch, X_batch)\n",
    "        loss = (recon_loss * self.settings.kl_alpha) + kl_loss\n",
    "\n",
    "        # Backwards Propagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        del X_batch, ygt_batch, mu_batch, var_batch, z_batch, X_fake_batch\n",
    "        return {'loss': loss, 'kl_loss': kl_loss, 'recon_loss': recon_loss}\n",
    "\n",
    "    # Functionality called upon the End of a Batch Training Step\n",
    "    def on_train_batch_end(self, loss, batch, batch_idx):\n",
    "\n",
    "        # Loss Values Update\n",
    "        self.train_loss = self.train_loss + loss['loss'].item()\n",
    "        self.train_kl_loss = self.train_kl_loss + loss['kl_loss'].item()\n",
    "        self.train_recon_loss = self.train_recon_loss + loss['recon_loss'].item()\n",
    "\n",
    "        # Last Batch's Example Original Image Saving\n",
    "        if batch_idx == self.train_batches - 1:\n",
    "            self.X_example, self.y_example = batch\n",
    "            self.y_example = self.y_example[-1, :]\n",
    "            self.X_example = self.X_example[-1, :, :, :]\n",
    "            self.X_example = self.X_example.view(1, self.settings.num_channel,\n",
    "                        self.settings.img_shape, self.settings.img_shape)\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Example Original vs Reconstructed Example Image Plotting Function\n",
    "    def img_plot(self, num_epochs: int = 0):\n",
    "\n",
    "        # Original vs Reconstruced Image Sampling\n",
    "        mu_example, var_example, z_example, self.X_fake_example = self.model(self.X_example)\n",
    "        self.X_example = self.X_example.view(self.settings.img_shape, self.settings.img_shape)\n",
    "        self.X_fake_example = self.X_fake_example.view(self.settings.img_shape, self.settings.img_shape)\n",
    "        del mu_example, var_example, z_example\n",
    "\n",
    "        # Original Example Image Subplot\n",
    "        figure = plt.figure(num_epochs, figsize = (60, 60))\n",
    "        plt.tight_layout(); plt.title(f'Epoch #{num_epochs}')\n",
    "        plt.subplot(2, 1, 1, title = 'Original')\n",
    "        plt.xticks([]); plt.yticks([]); plt.grid(False); plt.tight_layout()\n",
    "        plt.imshow(self.X_example.detach().numpy(), cmap = plt.cm.binary)\n",
    "\n",
    "        # Reconstructed Example Image Subplot\n",
    "        plt.subplot(2, 1, 2, title = 'Reconstruction')\n",
    "        plt.xticks([]); plt.yticks([]); plt.grid(False)\n",
    "        plt.imshow(self.X_fake_example.detach().numpy(), cmap = plt.cm.binary)\n",
    "        return figure\n",
    "\n",
    "    # Functionality called upon the End of a Training Epoch\n",
    "    def on_train_epoch_end(self):\n",
    "\n",
    "        # Learning Rate Decay\n",
    "        if (self.trainer.current_epoch + 1) in self.lr_decay_epochs:\n",
    "            self.lr_schedule.step()\n",
    "\n",
    "        # Loss Value Updating (Batch Division)\n",
    "        num_epochs = self.past_epochs + self.current_epoch\n",
    "        self.train_loss = self.train_loss / self.train_batches\n",
    "        self.train_kl_loss = self.train_kl_loss / self.train_batches\n",
    "        self.train_recon_loss = self.train_recon_loss / self.train_batches\n",
    "\n",
    "        # TensorBoard Logger Model Visualizer, Update for Scalar Values & Example Image Plotting\n",
    "        if num_epochs == 0:\n",
    "            self.train_logger.experiment.add_graph(self.model, self.X_example)\n",
    "        self.train_logger.experiment.add_scalar(\"Training Loss\", self.train_loss, num_epochs)\n",
    "        self.train_logger.experiment.add_scalar(\"Kullback Leibler Divergence\", self.train_kl_loss, num_epochs)\n",
    "        self.train_logger.experiment.add_scalar(\"Image Reconstruction Loss\", self.train_recon_loss, num_epochs)\n",
    "        plot = self.img_plot(num_epochs)\n",
    "        self.train_logger.experiment.add_figure(\"Original vs Reconstruction\", plot, num_epochs)\n",
    "\n",
    "        # Model Checkpoint Saving\n",
    "        torch.save({'ModelSD': self.model.state_dict(),\n",
    "                    'OptimizerSD': self.optimizer.state_dict(),\n",
    "                    'Training Epochs': num_epochs,\n",
    "                    'RNG State': torch.get_rng_state()},\n",
    "                    self.model_filepath)\n",
    "\n",
    "    ##############################################################################################\n",
    "    # ------------------------------------ Validation Script -------------------------------------\n",
    "    ##############################################################################################\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Linear** *VAE*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim: int = 128,\n",
    "        img_shape: int = 96\n",
    "    ):\n",
    "        \n",
    "        #\n",
    "        super(Encoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.img_shape = img_shape\n",
    "\n",
    "        #\n",
    "        self.fc1 = nn.Linear(img_shape * img_shape, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, latent_dim * 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.img_shape * self.img_shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        mean, logvar = x[:, :self.latent_dim], x[:, self.latent_dim:]\n",
    "        return mean, logvar\n",
    "\n",
    "#\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    #\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim: int = 128,\n",
    "        img_shape: int = 96\n",
    "    ):\n",
    "\n",
    "        #\n",
    "        super(Decoder, self).__init__()\n",
    "        self.img_shape = img_shape\n",
    "\n",
    "        #\n",
    "        self.fc1 = nn.Linear(latent_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.fc3 = nn.Linear(512, img_shape * img_shape)\n",
    "\n",
    "    #\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x.view(-1, 1, self.img_shape, self.img_shape)\n",
    "\n",
    "#\n",
    "class All4One(nn.Module):\n",
    "\n",
    "    #\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim: int = 128,\n",
    "        img_shape: int = 96\n",
    "    ):\n",
    "\n",
    "        #\n",
    "        super(All4One, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim, img_shape)\n",
    "        self.decoder = Decoder(latent_dim, img_shape)\n",
    "\n",
    "    \n",
    "    # Latent Space Reparametrization\n",
    "    @staticmethod\n",
    "    def reparam(mean, logvar):\n",
    "        std = torch.exp(logvar / 2)\n",
    "        epsilon = torch.randn_like(std)\n",
    "        return epsilon * std + mean\n",
    "\n",
    "    # All4One VAE Application Function\n",
    "    def forward(\n",
    "        self,\n",
    "        X: np.ndarray or torch.Tensor       # 3D Latent Representation Input\n",
    "    ):\n",
    "\n",
    "        # Forward Propagation in VAE Architecture\n",
    "        mu, var = self.encoder(X)\n",
    "        z = self.reparam(mu, var)\n",
    "        X_fake = self.decoder(z)\n",
    "        return mu, var, z, X_fake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE()\n",
    "X = torch.rand(5, 1, 96, 96)\n",
    "mu, var, z, X_fake = vae(X)\n",
    "plt.subplot(2, 1, 1); plt.imshow(X[0, 0, :, :])\n",
    "plt.subplot(2, 1, 2); plt.imshow(X_fake[0, 0, :, :].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE Model Training, Validation & Testing Script Class\n",
    "class LitAll4One(pl.LightningModule):\n",
    "\n",
    "    ##############################################################################################\n",
    "    # ----------------------------------- Model & Dataset Setup ----------------------------------\n",
    "    ##############################################################################################\n",
    "\n",
    "    # Constructor / Initialization Function\n",
    "    def __init__(\n",
    "        self,\n",
    "        settings: argparse.ArgumentParser,              # Model Settings & Parametrizations\n",
    "    ):\n",
    "\n",
    "        # Class Variable Logging\n",
    "        super().__init__()\n",
    "        self.settings = settings\n",
    "        self.lr_decay_epochs = [80, 140]                # Epochs for Learning Rate Decay\n",
    "\n",
    "        # Model Initialization\n",
    "        self.model = All4One(               latent_dim = settings.latent_dim,\n",
    "                                            img_shape = settings.img_shape)\n",
    "        self.optimizer = torch.optim.Adam(  self.model.parameters(),\n",
    "                                            lr = self.settings.base_lr,\n",
    "                                            weight_decay = self.settings.weight_decay, )\n",
    "        self.recon_criterion = nn.MSELoss(); self.past_epochs = 0\n",
    "\n",
    "        # Existing Model Checkpoint Loading\n",
    "        self.model_filepath = Path(f\"{self.settings.save_folderpath}/V{self.settings.model_version}/All4One (V{self.settings.model_version}).pth\")\n",
    "        if self.settings.model_version != 0 and self.model_filepath.exists():\n",
    "\n",
    "            # Checkpoint Fixing (due to the use of nn.DataParallel)\n",
    "            print(f\"DOWNLOADING All4One 2D VAE (Version {self.settings.model_version})\")\n",
    "            checkpoint = torch.load(self.model_filepath); self.checkpoint_fix = dict()\n",
    "            for sd, sd_value in checkpoint.items():\n",
    "                if sd == 'ModelSD' or sd == 'OptimizerSD':\n",
    "                    self.checkpoint_fix[sd] = OrderedDict()\n",
    "                    for key, value in checkpoint[sd].items():\n",
    "                        if key[0:7] == 'module.':\n",
    "                            self.checkpoint_fix[sd][key[7:]] = value\n",
    "                        else: self.checkpoint_fix[sd][key] = value\n",
    "                else: self.checkpoint_fix[sd] = sd_value\n",
    "            \n",
    "            # Application of Checkpoint's State Dictionary\n",
    "            self.model.load_state_dict(self.checkpoint_fix['ModelSD'])\n",
    "            self.optimizer.load_state_dict(self.checkpoint_fix['OptimizerSD'])\n",
    "            self.past_epochs = self.checkpoint_fix['Training Epochs']\n",
    "            torch.set_rng_state(self.checkpoint_fix['RNG State'])\n",
    "            del checkpoint\n",
    "        self.lr_schedule = torch.optim.lr_scheduler.ExponentialLR(  self.optimizer,     # Learning Rate Decay\n",
    "                                                    gamma = self.settings.lr_decay)     # in Chosen Epochs\n",
    "        self.model = nn.DataParallel(self.model.to(self.settings.device))\n",
    "        \n",
    "    # Optimizer Initialization Function\n",
    "    def configure_optimizers(self): return super().configure_optimizers()\n",
    "\n",
    "    # Foward Functionality\n",
    "    def forward(self, X): return self.model(X)\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Train Set DataLoader Download\n",
    "    def train_dataloader(self):\n",
    "        TrainTrainLoader = v3DMUDI.loader(  Path(f\"{self.settings.data_folderpath}\"),\n",
    "                                            dim = 2, version = self.settings.data_version,\n",
    "                                            #set_ = 'Test', mode_ = 'Train')\n",
    "                                            set_ = 'Train', mode_ = 'Train')\n",
    "        self.train_batches = len(TrainTrainLoader)\n",
    "        return TrainTrainLoader\n",
    "    \n",
    "    # Validation Set DataLoader Download\n",
    "    def val_dataloader(self):\n",
    "        TrainValLoader = v3DMUDI.loader(Path(f\"{self.settings.data_folderpath}\"),\n",
    "                                        dim = 2, version = self.settings.data_version,\n",
    "                                        #set_ = 'Test', mode_ = 'Train')\n",
    "                                        set_ = 'Train', mode_ = 'Val')\n",
    "        self.val_batches = len(TrainValLoader)\n",
    "        return TrainValLoader\n",
    "\n",
    "    # Test Set DataLoader Download\n",
    "    def test_dataloader(self):\n",
    "        TestValLoader = v3DMUDI.loader( Path(f\"{self.settings.data_folderpath}\"),\n",
    "                                        dim = 2, version = self.settings.data_version,\n",
    "                                        set_ = 'Test', mode_ = 'Val')\n",
    "        self.test_batches = len(TestValLoader)\n",
    "        return TestValLoader\n",
    "\n",
    "    ##############################################################################################\n",
    "    # ------------------------------------- Training Script --------------------------------------\n",
    "    ##############################################################################################\n",
    "\n",
    "    # Functionality called upon the Start of Training\n",
    "    def on_train_start(self):\n",
    "        \n",
    "        # Model Training Mode Setup\n",
    "        self.model.train()\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        # TensorBoard Logger Initialization\n",
    "        self.train_logger = TensorBoardLogger(f'{self.settings.save_folderpath}/V{self.settings.model_version}', 'Training Performance')\n",
    "\n",
    "    # Functionality called upon the Start of Training Epoch\n",
    "    def on_train_epoch_start(self):\n",
    "        self.train_loss = 0\n",
    "        self.train_kl_loss = 0\n",
    "        self.train_recon_loss = 0\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Training Step / Batch Loop \n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        # Data Handling\n",
    "        X_batch, ygt_batch = batch\n",
    "        X_batch = X_batch.type(torch.float).to(self.settings.device)\n",
    "        #ygt_batch = ygt_batch.type(torch.float).to(self.settings.device)\n",
    "\n",
    "        # Forward Propagation & Loss Computation\n",
    "        mu_batch, var_batch, z_batch, X_fake_batch = self.model(X_batch)\n",
    "        kl_loss =  (-0.5 * (1 + var_batch - mu_batch ** 2 - torch.exp(var_batch)).sum(dim = 1)).mean(dim = 0)        \n",
    "        recon_loss = self.recon_criterion(X_fake_batch, X_batch)\n",
    "        loss = (recon_loss * self.settings.kl_alpha) + kl_loss\n",
    "\n",
    "        # Backwards Propagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        del X_batch, ygt_batch, mu_batch, var_batch, z_batch, X_fake_batch\n",
    "        return {'loss': loss, 'kl_loss': kl_loss, 'recon_loss': recon_loss}\n",
    "\n",
    "    # Functionality called upon the End of a Batch Training Step\n",
    "    def on_train_batch_end(self, loss, batch, batch_idx):\n",
    "\n",
    "        # Loss Values Update\n",
    "        self.train_loss = self.train_loss + loss['loss'].item()\n",
    "        self.train_kl_loss = self.train_kl_loss + loss['kl_loss'].item()\n",
    "        self.train_recon_loss = self.train_recon_loss + loss['recon_loss'].item()\n",
    "\n",
    "        # Last Batch's Example Original Image Saving\n",
    "        if batch_idx == self.train_batches - 1:\n",
    "            self.X_example, self.y_example = batch\n",
    "            self.y_example = self.y_example[-1, :]\n",
    "            self.X_example = self.X_example[-1, :, :, :]\n",
    "            self.X_example = self.X_example.view(1, self.settings.num_channel,\n",
    "                        self.settings.img_shape, self.settings.img_shape)\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Example Original vs Reconstructed Example Image Plotting Function\n",
    "    def img_plot(self, num_epochs: int = 0):\n",
    "\n",
    "        # Original vs Reconstruced Image Sampling\n",
    "        mu_example, var_example, z_example, self.X_fake_example = self.model(self.X_example)\n",
    "        self.X_example = self.X_example.view(self.settings.img_shape, self.settings.img_shape)\n",
    "        self.X_fake_example = self.X_fake_example.view(self.settings.img_shape, self.settings.img_shape)\n",
    "        del mu_example, var_example, z_example\n",
    "\n",
    "        # Original Example Image Subplot\n",
    "        figure = plt.figure(num_epochs, figsize = (60, 60))\n",
    "        plt.tight_layout(); plt.title(f'Epoch #{num_epochs}')\n",
    "        plt.subplot(2, 1, 1, title = 'Original')\n",
    "        plt.xticks([]); plt.yticks([]); plt.grid(False); plt.tight_layout()\n",
    "        plt.imshow(self.X_example.detach().numpy(), cmap = plt.cm.binary)\n",
    "\n",
    "        # Reconstructed Example Image Subplot\n",
    "        plt.subplot(2, 1, 2, title = 'Reconstruction')\n",
    "        plt.xticks([]); plt.yticks([]); plt.grid(False)\n",
    "        plt.imshow(self.X_fake_example.detach().numpy(), cmap = plt.cm.binary)\n",
    "        return figure\n",
    "\n",
    "    # Functionality called upon the End of a Training Epoch\n",
    "    def on_train_epoch_end(self):\n",
    "\n",
    "        # Learning Rate Decay\n",
    "        if (self.trainer.current_epoch + 1) in self.lr_decay_epochs:\n",
    "            self.lr_schedule.step()\n",
    "\n",
    "        # Loss Value Updating (Batch Division)\n",
    "        num_epochs = self.past_epochs + self.current_epoch\n",
    "        self.train_loss = self.train_loss / self.train_batches\n",
    "        self.train_kl_loss = self.train_kl_loss / self.train_batches\n",
    "        self.train_recon_loss = self.train_recon_loss / self.train_batches\n",
    "\n",
    "        # TensorBoard Logger Model Visualizer, Update for Scalar Values & Example Image Plotting\n",
    "        if num_epochs == 0:\n",
    "            self.train_logger.experiment.add_graph(self.model, self.X_example)\n",
    "        self.train_logger.experiment.add_scalar(\"Training Loss\", self.train_loss, num_epochs)\n",
    "        self.train_logger.experiment.add_scalar(\"Kullback Leibler Divergence\", self.train_kl_loss, num_epochs)\n",
    "        self.train_logger.experiment.add_scalar(\"Image Reconstruction Loss\", self.train_recon_loss, num_epochs)\n",
    "        plot = self.img_plot(num_epochs)\n",
    "        self.train_logger.experiment.add_figure(\"Original vs Reconstruction\", plot, num_epochs)\n",
    "\n",
    "        # Model Checkpoint Saving\n",
    "        torch.save({'ModelSD': self.model.state_dict(),\n",
    "                    'OptimizerSD': self.optimizer.state_dict(),\n",
    "                    'Training Epochs': num_epochs,\n",
    "                    'RNG State': torch.get_rng_state()},\n",
    "                    self.model_filepath)\n",
    "\n",
    "    ##############################################################################################\n",
    "    # ------------------------------------ Validation Script -------------------------------------\n",
    "    ##############################################################################################\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Main** *Scripts*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: c:\\Users\\pfernan2\\Desktop\\Experiments\\Autoencoders\\All4One\\lightning_logs\n",
      "\n",
      "  | Name            | Type         | Params\n",
      "-------------------------------------------------\n",
      "0 | model           | DataParallel | 9.8 M \n",
      "1 | recon_criterion | MSELoss      | 0     \n",
      "-------------------------------------------------\n",
      "9.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "9.8 M     Total params\n",
      "39.235    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee17afa7035490c99a5eb92ff329343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Dataset Access\n",
    "sys.path.append(f\"{data_settings.main_folderpath}/Dataset Reader\")\n",
    "from v3DMUDI import v3DMUDI\n",
    "\n",
    "# Dataset Version Creation\n",
    "#data = v3DMUDI(data_settings)\n",
    "#data.split(data_settings)\n",
    "#data.save()\n",
    "#v3DMUDI.label_unscale(data_settings.save_folderpath, version = data_settings.version, y = data.params)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "# Full All4One 2D VAE Model Class Importing\n",
    "#sys.path.append(model_settings.model_folderpath)\n",
    "#from Encoder import Encoder\n",
    "#from Decoder import Decoder\n",
    "#from All4OneVAE import All4One\n",
    "\n",
    "# Full All4One 2D VAE Model Training Importing\n",
    "#sys.path.append(model_settings.script_folderpath)\n",
    "#from LitAll4One import LitAll4One\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "# Model Initialization & Training\n",
    "vae = LitAll4One(model_settings)\n",
    "vae_trainer = pl.Trainer(   max_epochs = model_settings.num_epochs,\n",
    "                            devices = 1 if torch.cuda.is_available() else None,\n",
    "                            enable_progress_bar = True,\n",
    "                            callbacks = [pl.callbacks.TQDMProgressBar(refresh_rate = 1)])\n",
    "vae_trainer.fit(vae)\n",
    "%load_ext tensorboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7e2413ca9464f5b18ee008ec75e3890212b75ca17b4a3699f34f03bf3acaeea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
